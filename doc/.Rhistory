library("tibble")
##Fisrstly, we check all needed packages and configurate the enviroment.
########install packages and  enviromnet cofiguration
packages.used=c("rvest", "tibble", "qdap", "ggplot2",
"sentimentr", "gplots", "dplyr","gcookbook",
"tm", "syuzhet", "factoextra", "scales", "RColorBrewer","wordcloud",
"RANN", "tm", "topicmodels","beeswarm","cluster","tidytext")
# check packages that need to be installed.
packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
#Configerate JAVA_Home
Sys.setenv(JAVA_HOME='C:/Program Files/Java/jre1.8.0_121')
# load packages
library("rvest")
library("tibble")
library("qdap")
install.packages("glue")
##Fisrstly, we check all needed packages and configurate the enviroment.
########install packages and  enviromnet cofiguration
packages.used=c("rvest", "tibble", "qdap", "ggplot2",
"sentimentr", "gplots", "dplyr","gcookbook",
"tm", "syuzhet", "factoextra", "scales", "RColorBrewer","wordcloud",
"RANN", "tm", "topicmodels","beeswarm","cluster","tidytext")
# check packages that need to be installed.
packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
#Configerate JAVA_Home
Sys.setenv(JAVA_HOME='C:/Program Files/Java/jre1.8.0_121')
# load packages
library("rvest")
library("tibble")
library("qdap")
r.version()
version
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre7')
library("qdap")
library(rJava)
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre7')
library(rJava)
library("qdap")
install.packages("qdap")
library("qdap")
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre7')
library(rJava)
library("qdap")
knitr::opts_chunk$set(echo = TRUE)
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
corpus <- VCorpus(VectorSource(hm_data$cleaned_hm))%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, character(0))%>%
tm_map(stripWhitespace)
stemmed <- tm_map(corpus, stemDocument) %>%
tidy() %>%
select(text)
dict <- tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
data("stop_words")
word <- c("happy","ago","yesterday","lot","today","months","month",
"happier","happiest","last","week","past")
stop_words <- stop_words %>%
bind_rows(mutate(tibble(word), lexicon = "updated"))
completed <- stemmed %>%
mutate(id = row_number()) %>%
unnest_tokens(stems, text) %>%
bind_cols(dict) %>%
anti_join(stop_words, by = c("dictionary" = "word"))
completed <- completed %>%
group_by(stems) %>%
count(dictionary) %>%
mutate(word = dictionary[which.max(n)]) %>%
ungroup() %>%
select(stems, word) %>%
distinct() %>%
right_join(completed) %>%
select(-stems)
completed <- completed %>%
group_by(id) %>%
summarise(text = str_c(word, collapse = " ")) %>%
ungroup()
hm_data <- hm_data %>%
mutate(id = row_number()) %>%
inner_join(completed)
datatable(hm_data)
write_csv(hm_data, "../output/processed_moments.csv")
library(tidyverse)
library(tidytext)
library(DT)
library(scales)
library(wordcloud2)
library(gridExtra)
library(ngram)
library(shiny)
library(wordcloud)
hm_data <- read_csv("../output/processed_moments.csv")
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
hm_data <- hm_data %>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
parenthood,
reflection_period,
age,
country,
ground_truth_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
datatable(hm_data)
hm_data$num_age <-as.numeric(hm_data$age)
hm_data<-hm_data[is.na(hm_data$num_age)!=T,]
age_between_20_30 <- hm_data[hm_data$num_age>=20&hm_data$num_age<30,]
age_between_30_40 <- hm_data[hm_data$num_age>=30&hm_data$num_age<40,]
bag_of_words <-  hm_data %>%
unnest_tokens(word, text)
word_count <- bag_of_words %>%
count(word, sort = TRUE)
word_age_between_20_30 <-age_between_20_30 %>%
unnest_tokens(word, text)
word_count_age_between_20_30 <- word_age_between_20_30 %>%
count(word,sort = TRUE)
word_age_between_30_40 <-age_between_30_40 %>%
unnest_tokens(word, text)
word_count_age_between_30_40 <- word_age_between_30_40 %>%
count(word,sort = TRUE)
wordcloud(words=word_count_age_between_20_30$word,freq=word_count_age_between_20_30$n,
min.freq=10, max.words= 100, random.order = F, rot.per = .35, colors = brewer.pal(8,"Dark2"))
wordcloud(words=word_count_age_between_30_40$word,freq=word_count_age_between_30_40$n,
min.freq=10, max.words= 100, random.order = F, rot.per = .35, colors = brewer.pal(8,"Dark2"))
age_between_20_30_parenthood <- age_between_20_30[age_between_20_30$parenthood=="y",]
age_between_30_40_parenthood <- age_between_30_40[age_between_30_40$parenthood=="y",]
bag_of_words <-  hm_data %>%
unnest_tokens(word, text)
word_count <- bag_of_words %>%
count(word, sort = TRUE)
word_age_between_20_30_parenthood <-age_between_20_30_parenthood %>%
unnest_tokens(word, text)
word_count_age_between_20_30_parenthood <- word_age_between_20_30_parenthood %>%
count(word,sort = TRUE)
word_age_between_30_40_parenthood <-age_between_30_40_parenthood %>%
unnest_tokens(word, text)
word_count_age_between_30_40_parenthood <- word_age_between_30_40_parenthood %>%
count(word,sort = TRUE)
wordcloud(words=word_age_between_20_30_parenthood$word,freq=word_count_age_between_20_30_parenthood$n,
min.freq=10, max.words= 100, random.order = F, rot.per = .35, colors = brewer.pal(8,"Dark2"))
wordcloud(words=word_count_age_between_30_40_parenthood$word,freq=word_count_age_between_30_40_parenthood$n,
min.freq=10, max.words= 100, random.order = F, rot.per = .35, colors = brewer.pal(8,"Dark2"))
age_between_20_30_male <- age_between_20_30[age_between_20_30$gender=="m",]
age_between_30_40_male <- age_between_30_40[age_between_30_40$gender=="m",]
bag_of_words <-  hm_data %>%
unnest_tokens(word, text)
word_count <- bag_of_words %>%
count(word, sort = TRUE)
word_age_between_20_30_male <-age_between_20_30_male %>%
unnest_tokens(word, text)
word_count_age_between_20_30_male <- word_age_between_20_30_male %>%
count(word,sort = TRUE)
word_age_between_30_40_male <-age_between_30_40_male %>%
unnest_tokens(word, text)
word_count_age_between_30_40_male <- word_age_between_30_40_male %>%
count(word,sort = TRUE)
wordcloud(words=word_age_between_20_30_male$word,freq=word_count_age_between_20_30_male$n,
min.freq=10, max.words= 100, random.order = F, rot.per = .35, colors = brewer.pal(8,"Dark2"))
wordcloud(words=word_count_age_between_30_40_male$word,freq=word_count_age_between_30_40_male$n,
min.freq=10, max.words= 100, random.order = F, rot.per = .35, colors = brewer.pal(8,"Dark2"))
hm_bigrams <- hm_data %>%
filter(count != 1) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
bigram_counts <- hm_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
count(word1, word2, sort = TRUE)
ui <- navbarPage("What makes people happy?",
tabPanel("Overview",
titlePanel(h1("Most Frequent Occurrences",
align = "center")),
sidebarLayout(
sidebarPanel(
sliderInput(inputId = "topWordcloud",
label = "Number of terms for word cloud:",
min = 5,
max = 100,
value = 50),
br(),
br(),
checkboxInput(inputId = "topFreqB",
label = "Plot Bar Chart",
value = F),
sliderInput(inputId = "topBarchart",
label = "Number of terms for bar chart:",
min = 1,
max = 25,
value = 10),
br(),
br(),
checkboxInput(inputId = "topFreqN",
label = "Plot Network Graph",
value = F),
sliderInput(inputId = "topNetwork",
label = "Number of edges for network graph:",
min = 1,
max = 150,
value = 50)
),
mainPanel(
wordcloud2Output(outputId = "WC"),
plotOutput(outputId = "figure")
)
)
),
tabPanel("Individual Terms",
titlePanel(h1("Comparison of Proportions",
align = "center")),
sidebarLayout(
sidebarPanel(
selectInput(inputId = "attribute",
label = "Select the attribute:",
choices = c("Gender" = "gender",
"Marital Status" = "marital",
"Parenthood" = "parenthood",
"Reflection Period" = "reflection_period")
)
),
mainPanel(
plotOutput(outputId = "scatter")
)
)
),
tabPanel("Pair of Words",
titlePanel(h1("Most Frequent Bigrams",
align = "center")),
sidebarLayout(
sidebarPanel(
selectInput(inputId = "factor",
label = "Select the attribute:",
choices = c("Gender" = "gender",
"Marital Status" = "marital",
"Parenthood" = "parenthood",
"Reflection Period" = "reflection_period")
),
numericInput(inputId = "topBigrams",
label = "Number of top pairs to view:",
min = 1,
max = 25,
value = 10)
),
mainPanel(
plotOutput(outputId = "bar")
)
)
),
tabPanel("Data",
DT::dataTableOutput("table")
)
)
server <- function(input, output, session) {
pt1 <- reactive({
if(!input$topFreqB) return(NULL)
word_count %>%
slice(1:input$topBarchart) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
ylab("Word Frequency")+
coord_flip()
})
pt2 <- reactive({
if(!input$topFreqN) return(NULL)
bigram_graph <- bigram_counts %>%
slice(1:input$topNetwork) %>%
graph_from_data_frame()
set.seed(123)
x <- grid::arrow(type = "closed", length = unit(.1, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = x, end_cap = circle(.05, 'inches')) +
geom_node_point(color = "skyblue", size = 3) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
})
output$WC <- renderWordcloud2({
word_count %>%
slice(1:input$topWordcloud) %>%
wordcloud2(size = 0.6,
rotateRatio = 0)
})
output$figure <- renderPlot(height = 500, width = 500, {
ptlist <- list(pt1(),pt2())
ptlist <- ptlist[!sapply(ptlist, is.null)]
if(length(ptlist)==0) return(NULL)
lay <- rbind(c(1,1),
c(2,2))
grid.arrange(grobs = ptlist, layout_matrix = lay)
})
selectedAttribute <- reactive({
list(atr = input$attribute)
})
output$scatter <- renderPlot({
temp <- bag_of_words %>%
count(!!as.name(selectedAttribute()$atr), word) %>%
group_by(!!as.name(selectedAttribute()$atr)) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(!!as.name(selectedAttribute()$atr), proportion)
ggplot(temp,
aes_string(x = colnames(temp)[2], y = colnames(temp)[3]),
color = abs(colnames(temp)[3] - colnames(temp)[2])) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 1, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
theme(legend.position="none")
})
selectedBigram <- reactive({
list(var = input$factor)
})
output$bar <- renderPlot({
hm_bigrams %>%
count(!!as.name(selectedBigram()$var), bigram, sort = TRUE) %>%
group_by(!!as.name(selectedBigram()$var)) %>%
top_n(input$topBigrams) %>%
ungroup() %>%
mutate(bigram = reorder(bigram, n)) %>%
ggplot(aes(bigram, n, fill = !!as.name(selectedBigram()$var))) +
geom_col(show.legend = FALSE) +
facet_wrap(as.formula(paste("~", selectedBigram()$var)), ncol = 2, scales = "free") +
coord_flip()
})
output$table <- DT::renderDataTable({
DT::datatable(hm_data)
})
}
shinyApp(ui, server)
happy.moment=read.csv("../output/processed_moments.csv", stringsAsFactors = FALSE)
demographic=read.csv("../data/demographic.csv")
happy.moment=read.csv("../output/processed_moments.csv", stringsAsFactors = FALSE)
senselabel=read.csv("../data/senselabel.csv")
happy.moment=read.csv("../output/processed_moments.csv", stringsAsFactors = FALSE)
happy.moment=read.csv("../output/processed_moments.csv", stringsAsFactors = FALSE)
demographic=read.csv("../data/demographic.csv")
demographic=read.csv("../data/demographic.csv")
senselabel=read.csv("../data/senselabel.csv")
happy.moment=read.csv("../output/processed_moments.csv", stringsAsFactors = FALSE)
happy.moment=read.csv("../output/processed_moments.csv", stringsAsFactors = FALSE)
demographic=read.csv("../data/demographic.csv")
senselabel=read.csv("../data/senselabel.csv")
Happy.text <- happy.moment$text
docs <- Corpus(VectorSource(Happy.text))
dtm <- DocumentTermMatrix(docs)
rownames(dtm) <- paste(happy.moment$reflection_period, happy.moment$wid,
happy.moment$predicted_category, happy.moment$hmid, sep="_")
rowTotals <- apply(dtm , 1, sum)
rowTotals <- apply(dtm , 1, sum)
rownames(dtm) <- paste(happy.moment$reflection_period, happy.moment$wid,
happy.moment$predicted_category, happy.moment$hmid, sep="_")
rowTotals <- sapply(dtm , 1, sum)
rowTotals <- tapply(dtm , 1, sum)
rowTotals <- apply(dtm , 1, sum)
dtm  <- dtm[rowTotals> 0, ]
happy.moment=read.csv("../output/processed_moments.csv", stringsAsFactors = FALSE)
demographic=read.csv("../data/demographic.csv")
senselabel=read.csv("../data/senselabel.csv")
Happy.text <- happy.moment$text
docs <- Corpus(VectorSource(Happy.text))
dtm <- DocumentTermMatrix(docs)
rownames(dtm) <- paste(happy.moment$reflection_period, happy.moment$wid,
happy.moment$predicted_category, happy.moment$hmid, sep="_")
rowTotals <- apply(dtm , 1, sum)
happy.moment=read.csv("../output/processed_moments.csv", stringsAsFactors = FALSE)
demographic=read.csv("../data/demographic.csv")
senselabel=read.csv("../data/senselabel.csv")
Happy.text <- happy.moment$text
docs <- Corpus(VectorSource(Happy.text))
dtm <- DocumentTermMatrix(docs)
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
#Number of topics
k <- 10
#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart,
seed = seed, best=best,
burnin = burnin, iter = iter,
thin=thin))
install.packages("qdap")
library(qdap)
########Pre-processing the data#####
###we build the corup using our text.list
docs<-Corpus(VectorSource(unlist(text.list)))
lda?
?lad
？lda
?lda
library(CRANN)
install.packages("CRANN")
install.packages("CRAN")
library(CRAN)
library('CRAN')
?LDA
install.packages('MASS')
library(MASS)
#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart,
seed = seed, best=best,
burnin = burnin, iter = iter,
thin=thin))
#Run LDA using Gibbs sampling
ldaOut <-lda(dtm, k, method="Gibbs", control=list(nstart=nstart,
seed = seed, best=best,
burnin = burnin, iter = iter,
thin=thin))
m <- hm_data[hm_data$gender=="m",]
m%>%
unnest_tokens(word, text)%>%
select(word)%>%
inner_join(get_sentiments("nrc"))%>%
count(word,sentiment, sort=T)%>%
ungroup()%>%
group_by(sentiment)%>%
top_n(10)%>%
ungroup()%>%
mutate(word=reorder(word, n))%>%
ggplot(aes(word, n, fill=sentiment))+
geom_col(show.legend = F)+
facet_wrap(~sentiment,scales = "free_y")+
labs(y="conttribution",
x=NULL)+
coord_flip()
library(tidyverse)
library(tidytext)
library(DT)
library(scales)
library(wordcloud2)
library(gridExtra)
library(ngram)
library(shiny)
library(wordcloud)
hm_data <- read_csv("../output/processed_moments.csv")
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
hm_data <- hm_data %>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
parenthood,
reflection_period,
age,
country,
ground_truth_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
knitr::opts_chunk$set(echo = TRUE)
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
corpus <- VCorpus(VectorSource(hm_data$cleaned_hm))%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, character(0))%>%
tm_map(stripWhitespace)
stemmed <- tm_map(corpus, stemDocument) %>%
tidy() %>%
select(text)
stemmed <- tm_map(corpus, stemDocument) %>%
tidy() %>%
select(text)
dict <- tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
q()
